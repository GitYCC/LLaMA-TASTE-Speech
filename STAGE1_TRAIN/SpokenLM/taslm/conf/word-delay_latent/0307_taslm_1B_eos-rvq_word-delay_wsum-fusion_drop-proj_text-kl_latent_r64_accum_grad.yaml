exp_name: 0307_taslm_1B_eos-rvq_word-delay_wsum-fusion_drop-proj_text-kl_latent_r64_accum_grad

# load_from_pretrained_dir: /proj/mtklmadm/dev/mtk53678/rtslm_storage/code/rtslm/SpokenLM/taslm/exp/0303_taslm_1B_eos-rvq_word-delay_gated-fusion_drop-proj_text-kl_latent_r64/checkpoint-18000
# resume_dir: /proj/mtklmadm/dev/mtk53678/rtslm_storage/code/rtslm/SpokenLM/taslm/exp/0303_taslm_1B_eos-rvq_word-delay_gated-fusion_drop-proj_text-kl_latent_r64/checkpoint-18000
exp_root: /proj/mtklmadm/dev/mtk53678/rtslm_storage/code/rtslm/SpokenLM/taslm/exp
tensorboard_root: /proj/mtklmadm/dev/mtk53678/rtslm_storage/code/rtslm/SpokenLM/taslm/tb

slm_config_dict:
    speech_token_size: 512
    speech_vocab_size: 512
    speech_num_channels: 4
    fusion_method: 'weighted_sum'
    fusion_kwargs:
        dropout_speech: 0.0
        normalize_speech: false
    speech_token_type: 'taste_0207_stg2_eos_rvq-d256-l4-k512_sum_smaller-lr'
    speech_tokenizer_pretrained_dir: /proj/mtklmadm/dev/mtk53678/rtslm_storage/code/rtslm/CosyVoice/examples/emilia/taste/exp/llm/torch_ddp/stage2/0207_stg2_eos_rvq-d256-l4-k512_sum_smaller-lr
    speech_tokenizer_hidden_size: 256
    speech_embed_directly_use_rvq: true
    speech_loss_apply_mask: true
    speech_labels_apply_quantization: false
    speech_token_adopt_latent_sampling: true
    speech_latent_sampler_kwargs:
        b_logvar_is_linear: false
    speech_tokenizer_rvq_kwargs: #copied from the token config
        dim: 256
        num_quantizers: 4
        codebook_dim: 256
        codebook_size: 512
        decay: 0.99
        quantize_dropout: true
        kmeans_init: False
    speech_multi_channel_loss_decay_factor: 0.52
    text_conduct_kl_loss: true
    # ====================================
    llama_pretrained_dir: /proj/mtklmadm/models/mtk53678/Llama-3.2-1B
    # llama_use_liger_kernel: true
    attn_implementation: flash_attention_2 # use sdpa for non-causal attn


use_lora: true
lora_model_dir:
lora_r: 64
lora_alpha: 64
lora_dropout: 0.1
lora_target_linear: true
lora_target_modules: 
lora_fan_in_fan_out:
lora_modules_to_save:
lora_linear_names_to_skip: 
   - lm_head

modules_to_finetune:
   - speech_embed_tokens
   - speech_head


train_data_list: /proj/mtklmadm/dev/mtk53678/rtslm_storage/code/rtslm/SpokenLM/taslm/data/0207_eos/train.data.list
eval_data_list: /proj/mtklmadm/dev/mtk53678/rtslm_storage/code/rtslm/SpokenLM/taslm/data/0207_eos/dev.data.list

collate_fn_name: pad_seq_collate_fn_for_taste
collate_fn_kwargs:
    map_orig_eos_to_special_token_id: 128001 # you have to set repeat token to true for using two special tokens
    ensure_alignment: false
    add_bos: true
    add_eos: true
    force_recompute_delayed: true
    use_delayed_token: true
    speech_pad_idx: 0
    speech_bos_idx: 0
    speech_eos_idx: 0
    speech_token_column_name: 'llm_aligned_taste_token_ids'
    text_token_column_name: 'llm_text_token_ids'
    # add_spk_emb: false

actual_batch_size: 768
micro_batch_size: 48
gradient_accumulation_steps: 2 # use ds config
test_micro_batch_size: 48
test_on_begin: true
num_epochs: 3
logging_steps: 100
eval_steps: 2000
eval_epochs:
optimizer: adamw_torch
lr_scheduler: cosine
warmup_steps: 1000
# warmup_steps: 10
weight_decay: 0.01
learning_rate: 1e-5
max_grad_norm: 1.0 # use ds config
gradient_checkpointing: true
use_liger_kernel: true
use_bf16: true

# deepspeed: 
# deepspeed: ./deepspeed_conf/zero1.json
deepspeed: ./deepspeed_conf/zero2.json
# deepspeed: ./deepspeed_configs/zero2.json

# special_tokens:
#    pad_token: <|end_of_text|>

seed: 42
