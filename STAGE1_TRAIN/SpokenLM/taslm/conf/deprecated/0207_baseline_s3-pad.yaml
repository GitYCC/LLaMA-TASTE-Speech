exp_name: baseline_s3-pad

exp_root: /proj/mtklmadm/dev/mtk53678/rtslm_storage/code/rtslm/SpokenLM/taslm/exp
tensorboard_root: /proj/mtklmadm/dev/mtk53678/rtslm_storage/code/rtslm/SpokenLM/taslm/tb

slm_config_dict:
    speech_token_size: 4096
    speech_num_channels: 1
    fusion_method: 'addition'
    speech_token_type: 's3'
    llama_pretrained_dir: /proj/mtklmadm/models/Llama-3.2-3B
    attn_implementation: flash_attention_2


use_lora: true
lora_model_dir:
lora_r: 16
lora_alpha: 32
lora_dropout: 0.1
lora_target_linear: true
lora_target_modules: 
lora_fan_in_fan_out:
lora_modules_to_save:

modules_to_finetune:
   - speech_embed_tokens
   - speech_head


train_data_list: /proj/mtklmadm/dev/mtk53678/rtslm_storage/code/rtslm/SpokenLM/taslm/data/train.data.list
eval_data_list: /proj/mtklmadm/dev/mtk53678/rtslm_storage/code/rtslm/SpokenLM/taslm/data/dev.data.list

collate_fn_kwargs:
    ensure_alignment: false
    add_bos: true
    add_eos: true
    speech_token_column_name: 's3_token_ids'
    text_token_column_name: 'llm_text_token_ids'

actual_batch_size: 64
micro_batch_size: 8
gradient_accumulation_steps: 1
test_micro_batch_size: 8
test_on_begin: false
num_epochs: 3
logging_steps: 100
eval_steps: 10000
eval_epochs:
optimizer: adamw_torch
lr_scheduler: cosine
warmup_steps: 1000
weight_decay: 0.0
learning_rate: 1e-4
max_grad_norm: 1.0
gradient_checkpointing: true
use_liger_kernel: true

deepspeed: ./deepspeed_conf/zero2.json
# deepspeed: ./deepspeed_configs/zero2.json
# deepspeed: ./deepspeed_conf/zero2.json

# special_tokens:
#    pad_token: <|end_of_text|>

seed: 42
