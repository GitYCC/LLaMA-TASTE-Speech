exp_name: 0213_baseline_1B_s3-pad_lora-r32

resume_dir: /proj/mtklmadm/dev/mtk53678/rtslm_storage/code/rtslm/SpokenLM/taslm/exp/0213_baseline_1B_s3-pad_lora-r32/checkpoint-70000
exp_root: /proj/mtklmadm/dev/mtk53678/rtslm_storage/code/rtslm/SpokenLM/taslm/exp
tensorboard_root: /proj/mtklmadm/dev/mtk53678/rtslm_storage/code/rtslm/SpokenLM/taslm/tb

slm_config_dict:
    speech_token_size: 4096
    speech_num_channels: 1
    fusion_method: 'addition'
    speech_token_type: 's3'
    llama_pretrained_dir: /proj/mtklmadm/models/mtk53678/Llama-3.2-1B
    # llama_use_liger_kernel: true
    attn_implementation: flash_attention_2


use_lora: true
lora_model_dir:
lora_r: 32
lora_alpha: 32
lora_dropout: 0.1
lora_target_linear: true
lora_target_modules: 
lora_fan_in_fan_out:
lora_modules_to_save:

modules_to_finetune:
   - speech_embed_tokens
   - speech_head


train_data_list: /proj/mtklmadm/dev/mtk53678/rtslm_storage/code/rtslm/SpokenLM/taslm/data/train.data.list
eval_data_list: /proj/mtklmadm/dev/mtk53678/rtslm_storage/code/rtslm/SpokenLM/taslm/data/dev.data.list

collate_fn_name: pad_seq_collate_fn
collate_fn_kwargs:
    ensure_alignment: false
    add_bos: true
    add_eos: true
    speech_token_column_name: 's3_token_ids'
    text_token_column_name: 'llm_text_token_ids'

actual_batch_size: 96
micro_batch_size: 12
gradient_accumulation_steps: 1
test_micro_batch_size: 12
test_on_begin: true
num_epochs: 1
logging_steps: 100
eval_steps: 10000
eval_epochs:
optimizer: adamw_torch
lr_scheduler: cosine
warmup_steps: 10000
weight_decay: 0.01
learning_rate: 5e-5
max_grad_norm: 5.0
gradient_checkpointing: true
use_liger_kernel: true
use_bf16: true

deepspeed: ./deepspeed_conf/zero2.json
# deepspeed: ./deepspeed_configs/zero2.json
# deepspeed: ./deepspeed_conf/zero2.json

# special_tokens:
#    pad_token: <|end_of_text|>

seed: 42
